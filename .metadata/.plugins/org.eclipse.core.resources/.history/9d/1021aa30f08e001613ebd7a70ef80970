package detected_community
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

object realcommunity {
  def main(args: Array[String]): Unit = {
    readfile()
    val aa=Array("aa","bb")
    print(aa(1))
  }
  def readfile()=
  {
    val conf=new SparkConf
    conf.setMaster("local[2]").setAppName("test")
    val sc=new SparkContext(conf)
    val FileDate=sc.textFile("/home/hu/code/F_measureOld1/need/real/realcommunity.txt")
    val array=lines(FileDate)
    //X 表示是value 要转换的类型 y是value的类型 结合 形成 y :: x 我理解就是前面 是 String 后面是 List[String]
    //以key分组 返回的是 key ,排好序 的 表
    //val test=array.groupByKey()
    //test.foreach(println(_))
    //可以自己指定 要返回的value的形式
    val arrays=array.combineByKey(List(_),(x:List[String], y:String) => y :: x, (x:List[String], y:List[String]) => x ::: y)
    //arrays.foreach { n => println(n) }
    //返回 value 的 list 链表 RDD[List[String]]
    val iterator=arrays.values;
    //val coll=iterator.collect()
    val collectPairs=iterator.map(n=>collect(n))
    collectPairs.foreach { n => println(n) }
    //返回 scala 的 map 集合
   // val map=arrays.collectAsMap();
   // map.foreach(println(_))
  }
  
  def lines(rdd:RDD[String]):RDD[(String,String)]=
  {
    rdd.flatMap(split)
  }
  def split(line:String):Array[(String,String)]=
  {
   val array=line.split("\t")
   val array1=array(1).split(" ")
   val array2=array1.map { node => (node,array(0)) }
   return array2
  }
  def collect(list:List[String]):List[String]=
  {
    list.foreach { n =>println(n) }
   val collects :List[Nothing]=List()
   //用for循环吧
   val length=list.length
   var pair=""
   for(i<-1 until(length))
   {
     val first=list(i)
     for(j<-i+1 until(length))
     {
       val second=list(j)
       if(first>second)
       {
       pair=first+"_"+second;
       }
       else
       {
         pair=second+"_"+first
       }
     }
   }
   //val collectPair=list.reduce((a,b)=>(a+"_"+b))
   //添加元素
   print(pair)
   pair :: collects
   return collects
  }
}